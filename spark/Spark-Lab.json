{"paragraphs":[{"text":"%md\n# Creating DataFrames","dateUpdated":"2018-12-18T20:50:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating DataFrames</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823278_1883574264","id":"20161121-211804_261745742","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1846","user":"anonymous","dateFinished":"2018-12-18T20:50:18+0000","dateStarted":"2018-12-18T20:50:18+0000"},{"text":"%pyspark\n\n# spark is an existing SparkSession\ndf = spark.read.json(\"/data/people.json\")\n# Displays the content of the DataFrame to stdout\ndf.show()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+","dateUpdated":"2018-12-18T20:50:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{"name":"","spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823290_1891269242","id":"20161121-211505_204906499","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1847","user":"anonymous","dateFinished":"2018-12-18T20:50:19+0000","dateStarted":"2018-12-18T20:50:18+0000"},{"text":"%md\n# Untyped Dataset Operations (aka DataFrame Operations)","dateUpdated":"2018-12-18T20:50:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Untyped Dataset Operations (aka DataFrame Operations)</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823291_1890884494","id":"20161121-212243_1042559294","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1848","user":"anonymous","dateFinished":"2018-12-18T20:50:19+0000","dateStarted":"2018-12-18T20:50:19+0000"},{"text":"%pyspark\n\n# spark, df are from the previous example\n# Print the schema in a tree format\ndf.printSchema()\n# root\n# |-- age: long (nullable = true)\n# |-- name: string (nullable = true)\n\n# Select only the \"name\" column\ndf.select(\"name\").show()\n# +-------+\n# |   name|\n# +-------+\n# |Michael|\n# |   Andy|\n# | Justin|\n# +-------+\n\n# Select everybody, but increment the age by 1\ndf.select(df['name'], df['age'] + 1).show()\n# +-------+---------+\n# |   name|(age + 1)|\n# +-------+---------+\n# |Michael|     null|\n# |   Andy|       31|\n# | Justin|       20|\n# +-------+---------+\n\n# Select people older than 21\ndf.filter(df['age'] > 21).show()\n# +---+----+\n# |age|name|\n# +---+----+\n# | 30|Andy|\n# +---+----+\n\n# Count people by age\ndf.groupBy(\"age\").count().show()\n# +----+-----+\n# | age|count|\n# +----+-----+\n# |  19|    1|\n# |null|    1|\n# |  30|    1|\n# +----+-----+","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+\n\n+-------+---------+\n|   name|(age + 1)|\n+-------+---------+\n|Michael|     null|\n|   Andy|       31|\n| Justin|       20|\n+-------+---------+\n\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n+----+-----+\n| age|count|\n+----+-----+\n|  19|    1|\n|null|    1|\n|  30|    1|\n+----+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823292_1888960749","id":"20161121-212329_269221065","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1849","user":"anonymous","dateFinished":"2018-12-18T20:50:20+0000","dateStarted":"2018-12-18T20:50:19+0000"},{"text":"%md\n# Running SQL Queries Programmatically","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Running SQL Queries Programmatically</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823292_1888960749","id":"20161121-212510_1424611231","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1850","user":"anonymous","dateFinished":"2018-12-18T20:50:19+0000","dateStarted":"2018-12-18T20:50:19+0000"},{"text":"%pyspark\n\n# Register the DataFrame as a SQL temporary view\ndf.createOrReplaceTempView(\"people\")\n\nsqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF.show()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823294_1889730247","id":"20161121-212653_1761768042","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1851","user":"anonymous","dateFinished":"2018-12-18T20:50:20+0000","dateStarted":"2018-12-18T20:50:19+0000"},{"text":"%md\n\n# Creating Datasets","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating Datasets</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823294_1889730247","id":"20161121-212717_908245571","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1852","user":"anonymous","dateFinished":"2018-12-18T20:50:19+0000","dateStarted":"2018-12-18T20:50:19+0000"},{"text":"%spark\n\n// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n// you can use custom classes that implement the Product interface\ncase class Person(name: String, age: Long)\n\n// Encoders are created for case classes\nval caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\ncaseClassDS.show()\n// +----+---+\n// |name|age|\n// +----+---+\n// |Andy| 32|\n// +----+---+\n\n// Encoders for most common types are automatically provided by importing spark.implicits._\nval primitiveDS = Seq(1, 2, 3).toDS()\nprimitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)\n\n// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\nval path =\"/data/people.json\"\nval peopleDS = spark.read.json(path).as[Person]\npeopleDS.show()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Person\ncaseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]\n+----+---+\n|name|age|\n+----+---+\n|Andy| 32|\n+----+---+\n\nprimitiveDS: org.apache.spark.sql.Dataset[Int] = [value: int]\nres60: Array[Int] = Array(2, 3, 4)\npath: String = /data/people.json\npeopleDS: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823295_1889345498","id":"20161121-213642_898338092","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1853","user":"anonymous","dateFinished":"2018-12-18T20:50:26+0000","dateStarted":"2018-12-18T20:50:20+0000"},{"text":"%md \n# Inferring the Schema Using Reflection","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Inferring the Schema Using Reflection</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823295_1889345498","id":"20161121-213745_455199652","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1854","user":"anonymous","dateFinished":"2018-12-18T20:50:19+0000","dateStarted":"2018-12-18T20:50:19+0000"},{"text":"%pyspark\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile\"/data/people.txt\")\nprint(lines.collect())\n\nparts = lines.map(lambda l: l.split(\",\"))\npeople = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n\n# Infer the schema, and register the DataFrame as a table.\nschemaPeople = spark.createDataFrame(people)\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nteenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n\n# The results of SQL queries are Dataframe objects.\n# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\nteenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\nfor name in teenNames:\n    print(name)\n# Name: Justin","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2651139953645538674.py\", line 344, in <module>\n    code = compile('\\n'.join(final_code), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)\n  File \"<stdin>\", line 1\n    lines = sc.textFile\"/data/people.txt\")\n                                        ^\nSyntaxError: invalid syntax\n"}]},"apps":[],"jobName":"paragraph_1545165823295_1889345498","id":"20161121-214219_563939179","dateCreated":"2018-12-18T20:43:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:1855","user":"anonymous","dateFinished":"2018-12-18T20:50:21+0000","dateStarted":"2018-12-18T20:50:21+0000"},{"text":"%md\n\n# Programmatically Specifying the Schema","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Programmatically Specifying the Schema</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823296_1875109789","id":"20161121-214254_1160849755","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1856","user":"anonymous","dateFinished":"2018-12-18T20:50:20+0000","dateStarted":"2018-12-18T20:50:19+0000"},{"text":"%pyspark\n\n# Import data types\nfrom pyspark.sql.types import *\n\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"/data/people.txt\")\nparts = lines.map(lambda l: l.split(\",\"))\n# Each line is converted to a tuple.\npeople = parts.map(lambda p: (p[0], p[1].strip()))\n\n# The schema is encoded in a string.\nschemaString = \"name age\"\n\nfields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)\n\n# Apply the schema to the RDD.\nschemaPeople = spark.createDataFrame(people, schema)\n\n# Creates a temporary view using the DataFrame\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nresults = spark.sql(\"SELECT name FROM people\")\n\nresults.show()\n# +-------+\n# |   name|\n# +-------+\n# |Michael|\n# |   Andy|\n# | Justin|\n# +-------+","dateUpdated":"2018-12-18T20:50:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823296_1875109789","id":"20161121-215434_1308115320","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1857","user":"anonymous","dateFinished":"2018-12-18T20:50:22+0000","dateStarted":"2018-12-18T20:50:21+0000"},{"text":"%md\n\n# Data Sources","dateUpdated":"2018-12-18T20:50:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Data Sources</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823297_1874725040","id":"20161121-215556_1513433460","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1858","user":"anonymous","dateFinished":"2018-12-18T20:50:20+0000","dateStarted":"2018-12-18T20:50:20+0000"},{"text":"%md \n# Generic Load/Save Functions","dateUpdated":"2018-12-18T20:50:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Generic Load/Save Functions</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823297_1874725040","id":"20161121-221300_1709976506","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1859","user":"anonymous","dateFinished":"2018-12-18T20:50:20+0000","dateStarted":"2018-12-18T20:50:20+0000"},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree('namesAndFavColors.parquet', ignore_errors=True)\n\ndf = spark.read.load(\"/data/users.parquet\")\ndf.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")","dateUpdated":"2018-12-18T20:50:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1545165823297_1874725040","id":"20161121-221324_1792501703","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1860","user":"anonymous","dateFinished":"2018-12-18T20:50:22+0000","dateStarted":"2018-12-18T20:50:21+0000"},{"text":"%pyspark\n\ndf = spark.read.load(\"namesAndFavColors.parquet\")\ndf.show()","dateUpdated":"2018-12-18T20:50:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------------+\n|  name|favorite_color|\n+------+--------------+\n|Alyssa|          null|\n|   Ben|           red|\n+------+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823298_1875879286","id":"20161121-221352_253824984","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1861","user":"anonymous","dateFinished":"2018-12-18T20:50:22+0000","dateStarted":"2018-12-18T20:50:22+0000"},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree('namesAndAges.parquet', ignore_errors=True)\n\n\ndf = spark.read.load(\"/data/people.json\", format=\"json\")\ndf.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")\n\ndf = spark.read.load(\"namesAndAges.parquet\")\ndf.show()","dateUpdated":"2018-12-18T20:50:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+----+\n|   name| age|\n+-------+----+\n|Michael|null|\n|   Andy|  30|\n| Justin|  19|\n+-------+----+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823298_1875879286","id":"20161121-221523_1944730815","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1862","user":"anonymous","dateFinished":"2018-12-18T20:50:23+0000","dateStarted":"2018-12-18T20:50:22+0000"},{"text":"%pyspark \n\ndf = spark.sql(\"SELECT * FROM parquet.`/data/users.parquet`\")\ndf.show()","dateUpdated":"2018-12-18T20:54:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823298_1875879286","id":"20161121-221645_1477557536","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1863","user":"anonymous","dateFinished":"2018-12-18T20:54:43+0000","dateStarted":"2018-12-18T20:54:43+0000"},{"text":"%md\n\n# Loading Data Programmatically","dateUpdated":"2018-12-18T20:50:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Loading Data Programmatically</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823299_1875494538","id":"20161121-222620_202866957","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1864","user":"anonymous","dateFinished":"2018-12-18T20:50:21+0000","dateStarted":"2018-12-18T20:50:21+0000"},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree(\"people.parquet\", ignore_errors=True)\n\npeopleDF = spark.read.json(\"/data/people.json\")\n\n# DataFrames can be saved as Parquet files, maintaining the schema information.\npeopleDF.write.parquet(\"people.parquet\")\n\n# Read in the Parquet file created above.\n# Parquet files are self-describing so the schema is preserved.\n# The result of loading a parquet file is also a DataFrame.\nparquetFile = spark.read.parquet(\"people.parquet\")\n\n# Parquet files can also be used to create a temporary view and then used in SQL statements.\nparquetFile.createOrReplaceTempView(\"parquetFile\")\nteenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\nteenagers.show()\n# +------+\n# |  name|\n# +------+\n# |Justin|\n# +------+","dateUpdated":"2018-12-18T20:50:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+\n|  name|\n+------+\n|Justin|\n+------+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823299_1875494538","id":"20161121-223150_132443134","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1865","user":"anonymous","dateFinished":"2018-12-18T20:50:24+0000","dateStarted":"2018-12-18T20:50:23+0000"},{"text":"%md\n\n# Schema Merging","dateUpdated":"2018-12-18T20:50:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Schema Merging</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823299_1875494538","id":"20161121-223224_183585777","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1866","user":"anonymous","dateFinished":"2018-12-18T20:50:21+0000","dateStarted":"2018-12-18T20:50:21+0000"},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree('data', ignore_errors=True)\n\nfrom pyspark.sql import Row\n\n# spark is from the previous example.\n# Create a simple DataFrame, stored into a partition directory\nsc = spark.sparkContext\n\nsquaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n                                  .map(lambda i: Row(single=i, double=i ** 2)))\nsquaresDF.write.parquet(\"data/test_table/key=1\")\n\n# Create another DataFrame in a new partition directory,\n# adding a new column and dropping an existing column\ncubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n                                .map(lambda i: Row(single=i, triple=i ** 3)))\ncubesDF.write.parquet(\"data/test_table/key=2\")\n\n# Read the partitioned table\nmergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\nmergedDF.printSchema()\n\n# The final schema consists of all 3 columns in the Parquet files together\n# with the partitioning column appeared in the partition directory paths.\n# root\n#  |-- double: long (nullable = true)\n#  |-- single: long (nullable = true)\n#  |-- triple: long (nullable = true)\n#  |-- key: integer (nullable = true)","dateUpdated":"2018-12-18T20:50:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- double: long (nullable = true)\n |-- single: long (nullable = true)\n |-- triple: long (nullable = true)\n |-- key: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1545165823300_1873570793","id":"20161121-223620_204262642","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1867","user":"anonymous","dateFinished":"2018-12-18T20:50:25+0000","dateStarted":"2018-12-18T20:50:24+0000"},{"text":"%pyspark\n\nmergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\nmergedDF.show()","dateUpdated":"2018-12-18T20:51:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------+------+---+\n|double|single|triple|key|\n+------+------+------+---+\n|  null|     9|   729|  2|\n|  null|    10|  1000|  2|\n|    16|     4|  null|  1|\n|    25|     5|  null|  1|\n|  null|     7|   343|  2|\n|  null|     8|   512|  2|\n|  null|     6|   216|  2|\n|     1|     1|  null|  1|\n|     9|     3|  null|  1|\n|     4|     2|  null|  1|\n+------+------+------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1545165823300_1873570793","id":"20161121-223720_479563238","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1868","user":"anonymous","dateFinished":"2018-12-18T20:50:26+0000","dateStarted":"2018-12-18T20:50:24+0000"},{"text":"%md\n\n# Spark SQL","dateUpdated":"2018-12-18T20:50:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark SQL</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545165823300_1873570793","id":"20161121-224455_578990309","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1869","user":"anonymous","dateFinished":"2018-12-18T20:50:21+0000","dateStarted":"2018-12-18T20:50:21+0000"},{"text":"%sql\nshow tables","dateUpdated":"2018-12-18T20:50:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"database\ttableName\tisTemporary\n\tparquetfile\ttrue\n\tpeople\ttrue\n"}]},"apps":[],"jobName":"paragraph_1545165823301_1873186044","id":"20161121-224034_728861532","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1870","user":"anonymous","dateFinished":"2018-12-18T20:50:26+0000","dateStarted":"2018-12-18T20:50:25+0000"},{"text":"%sql \n\nselect name, age from parquetfile where age > 0","dateUpdated":"2018-12-18T20:52:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"age","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"age","index":1,"aggr":"sum"}}},"helium":{}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nAndy\t30\nJustin\t19\n"}]},"apps":[],"jobName":"paragraph_1545165823301_1873186044","id":"20161121-224655_1255434563","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1871","user":"anonymous","dateFinished":"2018-12-18T20:52:49+0000","dateStarted":"2018-12-18T20:52:49+0000"},{"text":"","dateUpdated":"2018-12-18T20:47:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545165823301_1873186044","id":"20161121-224712_2007083219","dateCreated":"2018-12-18T20:43:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1872","user":"anonymous"}],"name":"Spark Lab Session","id":"2E16DYX52","angularObjects":{"2DZTP6D2D:shared_process":[],"2DY24CTP9:shared_process":[],"2DXP8V47F:shared_process":[],"2E1APAZ5R:shared_process":[],"2E1V6JJ91:shared_process":[],"2DYHP1KDT:shared_process":[],"2DX5YXM35:shared_process":[],"2DXG7A5UV:shared_process":[],"2DYBXK8WC:shared_process":[],"2DXMAHN63:shared_process":[],"2DXRJAN8F:shared_process":[],"2DYYZFDWW:shared_process":[],"2DX52NXAG:shared_process":[],"2DXR8TEX8:shared_process":[],"2DYAJQ68Z:shared_process":[],"2DZSHPWQ8:shared_process":[],"2DZGBMEXZ:shared_process":[],"2E1GP6NWC:shared_process":[],"2DYTN3YCM:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}